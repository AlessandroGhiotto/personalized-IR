{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other LLMs usage\n",
    "\n",
    "- Bi-encoder as first stage retriever\n",
    "- Cross-encoder\n",
    "- Causal LM\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we have some informal trial of other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR COLAB\n",
    "\n",
    "# !pip install -q condacolab\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "\n",
    "# !conda install -c pytorch faiss-gpu -y\n",
    "# !conda install -c conda-forge py-xgboost -y\n",
    "\n",
    "# !pip install --upgrade -q python-terrier\n",
    "# !pip install -q sentence_transformers ipdb emoji\n",
    "# !pip Install -q pyterrier-caching\n",
    "\n",
    "# !gdown 1HhgXzyEpsZNcenU9XhJuOYyDUKEzUse4\n",
    "# !unzip pir_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import faiss\n",
    "import joblib\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "# Hugging Face\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, QuantoConfig\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# TEXT PROCESSING\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# TERRIER\n",
    "from pyterrier.measures import *\n",
    "import pyterrier as pt\n",
    "from pyterrier_caching import RetrieverCache\n",
    "\n",
    "# Move to the parent directory\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "if not pt.java.started():\n",
    "    pt.utils.set_tqdm('notebook')\n",
    "    pt.java.init()\n",
    "\n",
    "\n",
    "# FILTER WARNINGS\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead\",\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\",\n",
    ")\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# SET SEED\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### UTILITY function\n",
    "### DISPLAY STYLED df\n",
    "# set with colors highest values in each column\n",
    "def display_styled(df, ignore_cols=[], color=\"#37614a\"):\n",
    "    \"\"\"ignore_cols: list of columns to not color\"\"\"\n",
    "    def highlight_max(s):\n",
    "        if s.name in ignore_cols:  # Skip styling for the 'Name' column\n",
    "            return ['' for _ in s]\n",
    "        is_max = s == s.max()\n",
    "        return [f'font-weight: bold; background-color: {color};' if v else '' for v in is_max]\n",
    "\n",
    "    styled_df = (\n",
    "        df.style\n",
    "        .apply(highlight_max, axis=0)  # Apply styling\n",
    "        .format({col: \"{:.3f}\" for col in df.select_dtypes(include='number').columns})  # Format numeric columns only\n",
    "    )\n",
    "    display(styled_df)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text, apply_stemmer=False, remove_stopwords=False):\n",
    "    # remove emojis\n",
    "    text = emoji.replace_emoji(text, \"\")\n",
    "    # remove links\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    # remove html tags\n",
    "    # text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    # lowercase verything\n",
    "    text = text.lower()\n",
    "    # remove backslashes\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    # remove special characters and punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # remove whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # remove leading and trailing whites\n",
    "    text = text.strip()\n",
    "    # apply spelling correction\n",
    "    # text = TextBlob(text).correct()\n",
    "    tokens = text.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    if apply_stemmer:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWERS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>writers_2010</td>\n",
       "      <td>TL;DRIf you're going to do present tense do it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>writers_2018</td>\n",
       "      <td>Your writing style is stream-of-consciousness,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>writers_2023</td>\n",
       "      <td>Place emphasis on uncomfortable things. Depend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          docno                                               text\n",
       "0  writers_2010  TL;DRIf you're going to do present tense do it...\n",
       "1  writers_2018  Your writing style is stream-of-consciousness,...\n",
       "2  writers_2023  Place emphasis on uncomfortable things. Depend..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERIES\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query_unprocessed</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academia_100305</td>\n",
       "      <td>What are CNRS research units and how are they ...</td>\n",
       "      <td>1106095</td>\n",
       "      <td>1513009820</td>\n",
       "      <td>cnr research unit staf centr nation de la rech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academia_100456</td>\n",
       "      <td>Is there a free (as in freedom) alternative to...</td>\n",
       "      <td>1106095</td>\n",
       "      <td>1513191752</td>\n",
       "      <td>free freedom altern publon review journal allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academia_103390</td>\n",
       "      <td>Search for StackExchange citations with Google...</td>\n",
       "      <td>1532620</td>\n",
       "      <td>1517935259</td>\n",
       "      <td>search stackexchang citat googl scholar possib...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qid                                  query_unprocessed  \\\n",
       "0  academia_100305  What are CNRS research units and how are they ...   \n",
       "1  academia_100456  Is there a free (as in freedom) alternative to...   \n",
       "2  academia_103390  Search for StackExchange citations with Google...   \n",
       "\n",
       "   user_id   timestamp                                              query  \n",
       "0  1106095  1513009820  cnr research unit staf centr nation de la rech...  \n",
       "1  1106095  1513191752  free freedom altern publon review journal allo...  \n",
       "2  1532620  1517935259  search stackexchang citat googl scholar possib...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRELS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academia_100305</td>\n",
       "      <td>academia_100217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academia_100456</td>\n",
       "      <td>academia_100462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academia_103390</td>\n",
       "      <td>academia_103391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qid            docno  label\n",
       "0  academia_100305  academia_100217      1\n",
       "1  academia_100456  academia_100462      1\n",
       "2  academia_103390  academia_103391      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0, docno = writers_2010\n"
     ]
    }
   ],
   "source": [
    "# COLLECTION OF DOCUMENTS (ANSWERS)\n",
    "def preprocess_corpus(df):\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['docno', 'text']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "corpus_df = preprocess_corpus(pd.read_json('PIR_data/answer_retrieval/subset_answers.json', orient='index'))\n",
    "\n",
    "# SAMPLES (QUERIES)\n",
    "def preprocess_queries_df(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df = df[['id', 'text', 'user_id', 'timestamp']]\n",
    "    df.columns = ['qid', 'query_unprocessed', 'user_id', 'timestamp']\n",
    "    df['query'] = df['query_unprocessed'].apply(lambda x: preprocess_text(x, apply_stemmer=True, remove_stopwords=True))\n",
    "    df['timestamp'] = df[\"timestamp\"].astype(int) // 10**9\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_queries = preprocess_queries_df('PIR_data/answer_retrieval/train/subset_data.jsonl')\n",
    "val_queries = preprocess_queries_df('PIR_data/answer_retrieval/val/subset_data.jsonl')\n",
    "test_queries = preprocess_queries_df('PIR_data/answer_retrieval/test/subset_data.jsonl')\n",
    "\n",
    "# QRELS\n",
    "def preprocess_qrels_df(path):\n",
    "    df = pd.read_json(path, orient='index').reset_index()\n",
    "    df.columns = ['qid', 'docno']\n",
    "    df['label'] = 1\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_qrels = preprocess_qrels_df('PIR_data/answer_retrieval/train/qrels.json')\n",
    "val_qrels = preprocess_qrels_df('PIR_data/answer_retrieval/val/qrels.json')\n",
    "test_qrels = preprocess_qrels_df('PIR_data/answer_retrieval/test/qrels.json')\n",
    "\n",
    "print(\"ANSWERS\")\n",
    "display(corpus_df.head(3))\n",
    "print(\"QUERIES\")\n",
    "display(train_queries.head(3))\n",
    "print(\"QRELS\")\n",
    "display(train_qrels.head(3)) \n",
    "\n",
    "# Create an index:docno dictionary for the corpus_df dataframe\n",
    "index_docno_dict = {index: row['docno'] for index, row in corpus_df.iterrows()}\n",
    "k = 0\n",
    "print(f\"index = {k}, docno = {index_docno_dict[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REPORTED FROM PREVIOUS NOTEBOOK\n",
    "### TAGS SCORE\n",
    "\n",
    "path = './index_sepqa/user_tags_full.joblib'\n",
    "USER_TAGS = joblib.load(path)\n",
    "\n",
    "def get_user_tags(user_id, timestamp, include_curr_timestamp, user_tags=USER_TAGS):\n",
    "    \"\"\"\n",
    "    Get the tags of a user at a given timestamp.\n",
    "\n",
    "    include_curr_timestamp: if True, the tags at the given timestamp are included.\n",
    "                            if False, the tags at the given timestamp are excluded.\n",
    "    \"\"\"\n",
    "    tags = set()\n",
    "    timestamp = int(timestamp)\n",
    "    \n",
    "    if include_curr_timestamp == False:\n",
    "        timestamp -= 1 # exclude the question at the given timestamp\n",
    "\n",
    "    # if the user_id is not in the user_tags dictionary, return an empty set (no profile for the user)\n",
    "    if user_id not in user_tags:\n",
    "        return tags\n",
    "    \n",
    "    for ts, user_tags in user_tags[user_id]:\n",
    "        if ts <= timestamp:\n",
    "            tags = tags.union(user_tags)\n",
    "        # the ts are sorted, so we can break when we reach the timestamp\n",
    "        else:\n",
    "            break\n",
    "    return tags\n",
    "\n",
    "def _get_tags_score(df, get_user_tags_fn=get_user_tags):\n",
    "    \"\"\"\n",
    "    get scores based on the tags of the user that asked the question\n",
    "    and the user that have written the answer.\n",
    "\n",
    "    used as argument of pyterrier.apply.doc_score()\n",
    "        =>  the input is a ranked documents dataframe (batch), by query\n",
    "            the output are the scores for each document in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_score(tags_uq, tags_ua):\n",
    "        \"\"\"\n",
    "        tags_uq: set of tags of the user that asked the question\n",
    "        tags_ua: set of tags of the user that wrote the answer\n",
    "        \"\"\"\n",
    "        return len(tags_uq.intersection(tags_ua)) / (len(tags_uq) + 1)\n",
    "\n",
    "    if not all(df['qid'] == df['qid'].iloc[0]):\n",
    "        assert \"Not all qids in the batch are equal\"\n",
    "    # user of the query\n",
    "    uq = df['user_id'].iloc[0]\n",
    "    # timestamp of the query\n",
    "    tq = df['timestamp'].iloc[0]\n",
    "    # get the tags of the user that asked the question\n",
    "    tags_uq = get_user_tags_fn(uq, tq, include_curr_timestamp=True)\n",
    "\n",
    "    # users that have written the answers\n",
    "    uaS = df['doc_user_id'].values\n",
    "    # get the tags of the users that have written the answers\n",
    "    tags_uaS = [get_user_tags_fn(ua, tq, include_curr_timestamp=False) for ua in uaS]\n",
    "    # compute the score for each answer\n",
    "    scores = [compute_score(tags_uq, tags_ua) for tags_ua in tags_uaS]\n",
    "    return scores\n",
    "\n",
    "\n",
    "##### REPORTED FROM PREVIOUS NOTEBOOK\n",
    "### NUM QUESTIONS\n",
    "\n",
    "\n",
    "def get_user_num_questions(user_id, timestamp, user_tags=USER_TAGS):\n",
    "    \"\"\"\n",
    "    Get the number of questions that a user have written at a given timestamp.\n",
    "    the current timestamp is included.\n",
    "    \"\"\"\n",
    "    num_questions = 0\n",
    "    timestamp = int(timestamp)\n",
    "    \n",
    "    # if the user_id is not in the user_tags dictionary, return 0 (no profile for the user)\n",
    "    if user_id not in user_tags:\n",
    "        return num_questions\n",
    "    \n",
    "    for ts, user_tags in user_tags[user_id]:\n",
    "        if ts <= timestamp:\n",
    "            num_questions += 1\n",
    "        # early stopping since the timestamps are sorted\n",
    "        else:\n",
    "            break\n",
    "    return num_questions\n",
    "\n",
    "# THE NUM_QUESTIONS IS REFERRED TO THE USER THAT ASKED THE QUESTION\n",
    "def _get_num_questions(df, get_user_num_questions_fn=get_user_num_questions):\n",
    "    \"\"\"\n",
    "    get the number of questions of the user that asked the question\n",
    "    \"\"\"\n",
    "    if not all(df['qid'] == df['qid'].iloc[0]):\n",
    "        assert \"Not all qids in the batch are equal\"\n",
    "    # user of the query\n",
    "    uq = df['user_id'].iloc[0]\n",
    "    # timestamp of the query\n",
    "    tq = df['timestamp'].iloc[0]\n",
    "    # get the number of questions of the user that asked the question\n",
    "    scores = [get_user_num_questions_fn(uq, tq)]\n",
    "    scores = scores * len(df) # repeat the score for each document in the batch\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bi-Encoder as first stage retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _transform_biencoder(df, FAISS_INDEX, biencoder_model, text_field='query_unprocessed', k=100, index_docno_dict=None):\n",
    "    \"\"\"\n",
    "    Transform queries with a biencoder model.\n",
    "\n",
    "    Used as an argument of pyterrier.apply.generic()\n",
    "        => The input is a dataframe of queries.\n",
    "        => The output is a dataframe of ranked documents.\n",
    "           The size of the output dataframe is the number of queries * k.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with queries.\n",
    "        FAISS_INDEX: FAISS index for document embeddings.\n",
    "        biencoder_model: Bi-encoder model for encoding queries.\n",
    "        text_field (str): Column name for query text in the dataframe.\n",
    "        k (int): Number of top results to return per query.\n",
    "        index_docno_dict (dict): Dictionary mapping FAISS indices to document IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with ranked documents for each query.\n",
    "    \"\"\"\n",
    "    def get_dense_scores(df):\n",
    "        \"\"\"\n",
    "        Get cosine similarity scores with a biencoder model using a FAISS FlatIndex.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe with query details.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with top-k document scores and ranks.\n",
    "        \"\"\"\n",
    "        if not all(df['qid'] == df['qid'].iloc[0]):\n",
    "            raise ValueError(\"Not all qids in the batch are equal.\")\n",
    "\n",
    "        # Get the query unprocessed text\n",
    "        query_text = df[text_field].iloc[0]\n",
    "\n",
    "        # Encode the query to get its embedding\n",
    "        query_embedding = biencoder_model.encode(query_text).astype('float32')\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "        # Search the FAISS index\n",
    "        scores, indices = FAISS_INDEX.search(np.array([query_embedding]), k)\n",
    "\n",
    "        # Expand the input dataframe for the top-k results\n",
    "        res_df = pd.concat([df] * k, ignore_index=True)\n",
    "\n",
    "        # Map FAISS indices to document IDs\n",
    "        docnos = [index_docno_dict[i] for i in indices[0]]\n",
    "        res_df['docno'] = docnos\n",
    "        res_df['score'] = scores[0]\n",
    "\n",
    "        return res_df\n",
    "\n",
    "    results = []\n",
    "    cols = ['Index'] + df.columns.tolist()\n",
    "\n",
    "    # Process each query row-by-row\n",
    "    # I get the top-k results for each query\n",
    "    for row in df.itertuples():\n",
    "        row_df = pd.DataFrame([row], columns=cols)\n",
    "        res_df = get_dense_scores(row_df)\n",
    "        results.append(res_df)\n",
    "\n",
    "    # Concatenate all results and add ranks\n",
    "    results_df = pd.concat(results, ignore_index=True)\n",
    "    return pt.model.add_ranks(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BM25\n",
    "path = \"./index_sepqa/index_bm25/data.properties\"\n",
    "bm25_index = pt.IndexFactory.of(path)\n",
    "\n",
    "bm25 = pt.terrier.Retriever(\n",
    "    bm25_index, \n",
    "    wmodel=\"BM25\", \n",
    "    controls={'c': 1.0, 'bm25.k_1': 2.5},\n",
    "    properties={\"termpipelines\": \"\"},\n",
    ") % 100\n",
    "\n",
    "### BI-ENCODER\n",
    "index_path = \"./index_sepqa/MiniLM_faiss_IndexFlatIP.index\"\n",
    "faiss_index = faiss.read_index(index_path)\n",
    "biencoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "transform_biencoder = partial(_transform_biencoder, FAISS_INDEX=faiss_index, biencoder_model=biencoder_model, k=100, index_docno_dict=index_docno_dict)\n",
    "bi_enc = pt.apply.generic(transform_biencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>AP@100</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.815</td>\n",
       "      <td>13.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-Encoder</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.941</td>\n",
       "      <td>12.257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    P@1    P@3  nDCG@3  nDCG@10  R@100  AP@100     mrt\n",
       "0        BM25  0.755  0.286   0.814    0.841  0.969   0.815  13.172\n",
       "1  Bi-Encoder  0.918  0.316   0.938    0.952  1.000   0.941  12.257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [P@1, P@3, nDCG@3, nDCG@10, R@100, MAP@100, 'mrt']\n",
    "\n",
    "res = pt.Experiment(\n",
    "    [bm25, bi_enc],\n",
    "    val_queries,\n",
    "    val_qrels,\n",
    "    names=[\"BM25\", \"Bi-Encoder\"],\n",
    "    eval_metrics=metrics,\n",
    ")\n",
    "display(res.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Pipeline with BiEncoder as first stage retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BM25\n",
    "path = \"./index_sepqa/index_bm25_users/data.properties\"\n",
    "bm25_index = pt.IndexFactory.of(path)\n",
    "bm25 = pt.terrier.Retriever(\n",
    "    bm25_index, \n",
    "    wmodel=\"BM25\", \n",
    "    controls={'c': 1.0, 'bm25.k_1': 2.5},\n",
    "    properties={\"termpipelines\": \"\"}, \n",
    "    metadata=[\"docno\", \"doc_user_id\"] # ADD doc_user_id TO THE METADATA TO BE RETRIEVED\n",
    ")\n",
    "norm_bm25 = bm25 >> pt.pipelines.PerQueryMaxMinScoreTransformer() ## NORMALIZE THE SCORES\n",
    "\n",
    "\n",
    "### BI-ENCODER\n",
    "transform_biencoder = partial(_transform_biencoder, FAISS_INDEX=faiss_index, biencoder_model=biencoder_model, k=100, index_docno_dict=index_docno_dict)\n",
    "bi_enc = pt.apply.generic(transform_biencoder)\n",
    "norm_bi_enc = bi_enc >> pt.pipelines.PerQueryMaxMinScoreTransformer()\n",
    "\n",
    "### TAGS-SCORE\n",
    "tags_score = pt.apply.doc_score(_get_tags_score, batch_size=64)\n",
    "norm_tags_score = tags_score >> pt.pipelines.PerQueryMaxMinScoreTransformer()\n",
    "\n",
    "### NUM QUESTIONS\n",
    "num_questions = pt.apply.doc_score(_get_num_questions, batch_size=64)\n",
    "def _score_wa(df, k=512):\n",
    "    \"\"\"We keep the tags score only if the user that asked the question has written at least k questions.\"\"\"\n",
    "    # FEATURES = [bi_enc, bm25, tags_score, num_questions]\n",
    "    num_questions = df['features'][-1]\n",
    "    if num_questions >= k:\n",
    "        weights = [0.7, 0.1, 0.2]\n",
    "    else:\n",
    "        weights = [0.9, 0.1, 0]\n",
    "\n",
    "    return np.dot(df['features'][:-1], weights)\n",
    "score_wa = partial(_score_wa, k=512)\n",
    "wa_with_heuristics = pt.apply.doc_score(score_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET doc_user_id\n",
    "path = './index_sepqa/subset_answers_with_users.json'\n",
    "corpus_df_WITH_USERS = pd.read_json(path, orient=\"records\", lines=True)\n",
    "corpus_df_WITH_USERS['doc_user_id'] = corpus_df_WITH_USERS['doc_user_id'].astype(str)\n",
    "DOC_USER_DICT = corpus_df_WITH_USERS.set_index('docno')['doc_user_id'].to_dict()\n",
    "\n",
    "# this block add the doc_user_id to the dataframe in the retrieval pipeline\n",
    "# here is done separately, because we are using the biencoder at first stage\n",
    "# with BM25 we retrieve it from the metadata (here we cant)\n",
    "get_doc_user_id = pt.apply.doc_user_id(lambda row: DOC_USER_DICT[row['docno']])\n",
    "\n",
    "\n",
    "### SCORE TO FEATURE\n",
    "# this block add score to the features\n",
    "# so I don't need to rerun the biencoder for the second stage\n",
    "# we also normalize it\n",
    "score_to_feature_norm = pt.apply.doc_score(lambda x: x['score'], batch_size=64) >> pt.pipelines.PerQueryMaxMinScoreTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERSONALIZED RERANKING WA with heuristics\n",
    "reranking_WAH = bi_enc % 100 \\\n",
    "                >> get_doc_user_id \\\n",
    "                >> score_to_feature_norm ** norm_bm25 ** norm_tags_score ** num_questions \\\n",
    "                >> wa_with_heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>AP@100</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.815</td>\n",
       "      <td>15.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-Encoder</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.941</td>\n",
       "      <td>12.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best model with Bi-Encoder as first stage</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.943</td>\n",
       "      <td>40.784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name    P@1    P@3  nDCG@3  nDCG@10  \\\n",
       "0                                       BM25  0.755  0.286   0.814    0.841   \n",
       "1                                 Bi-Encoder  0.918  0.316   0.938    0.952   \n",
       "2  Best model with Bi-Encoder as first stage  0.918  0.320   0.943    0.954   \n",
       "\n",
       "   R@100  AP@100     mrt  \n",
       "0  0.969   0.815  15.575  \n",
       "1  1.000   0.941  12.185  \n",
       "2  1.000   0.943  40.784  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [P@1, P@3, nDCG@3, nDCG@10, R@100, MAP@100, 'mrt']\n",
    "\n",
    "res = pt.Experiment(\n",
    "    [bm25, bi_enc, reranking_WAH],\n",
    "    val_queries,\n",
    "    val_qrels,\n",
    "    names=[\"BM25\", \"Bi-Encoder\", \"Best model with Bi-Encoder as first stage\"],\n",
    "    eval_metrics=metrics,\n",
    ")\n",
    "display(res.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>AP@100</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.843</td>\n",
       "      <td>13.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-Encoder</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.895</td>\n",
       "      <td>11.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best model with Bi-Encoder as first stage</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.911</td>\n",
       "      <td>40.793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name    P@1    P@3  nDCG@3  nDCG@10  \\\n",
       "0                                       BM25  0.806  0.286   0.836    0.861   \n",
       "1                                 Bi-Encoder  0.857  0.310   0.898    0.912   \n",
       "2  Best model with Bi-Encoder as first stage  0.878  0.310   0.908    0.923   \n",
       "\n",
       "   R@100  AP@100     mrt  \n",
       "0  0.969   0.843  13.482  \n",
       "1  1.000   0.895  11.614  \n",
       "2  1.000   0.911  40.793  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [P@1, P@3, nDCG@3, nDCG@10, R@100, MAP@100, 'mrt']\n",
    "\n",
    "res = pt.Experiment(\n",
    "    [bm25, bi_enc, reranking_WAH],\n",
    "    test_queries,\n",
    "    test_qrels,\n",
    "    names=[\"BM25\", \"Bi-Encoder\", \"Best model with Bi-Encoder as first stage\"],\n",
    "    eval_metrics=metrics,\n",
    ")\n",
    "display(res.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Next we have some fun methods, but they need to be refined. this is just a trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "cross_model = CrossEncoder(model_name, max_length=512, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_df is indexed by 'docno'\n",
    "corpus_df_docno = corpus_df.set_index('docno')\n",
    "\n",
    "def _get_cross_scores(df, cross_model, query_field='query_unprocessed', corpus_df=corpus_df_docno):\n",
    "    \"\"\"\n",
    "    get cosine similarity score with a crossencoder model\n",
    "\n",
    "    used as argument of pyterrier.apply.doc_score()\n",
    "        =>  the input is a ranked documents dataframe (batch), by query\n",
    "            the output are the scores for each document in the batch\n",
    "    \"\"\"\n",
    "    query_text = df[query_field].values\n",
    "    docno_list = df['docno'].tolist()\n",
    "    doc_text = corpus_df.reindex(docno_list)['text'].values\n",
    "\n",
    "    scores = cross_model.predict(list(zip(query_text, doc_text)))\n",
    "    return scores\n",
    "\n",
    "cross_scores = partial(_get_cross_scores, cross_model=cross_model)\n",
    "cross_enc = pt.apply.doc_score(cross_scores, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>AP@100</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.815</td>\n",
       "      <td>15.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-Encoder</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.941</td>\n",
       "      <td>11.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cross-Encoder</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.902</td>\n",
       "      <td>48.693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name    P@1    P@3  nDCG@3  nDCG@10  R@100  AP@100     mrt\n",
       "0           BM25  0.755  0.286   0.814    0.841  0.969   0.815  15.380\n",
       "1     Bi-Encoder  0.918  0.316   0.938    0.952  1.000   0.941  11.180\n",
       "2  Cross-Encoder  0.857  0.313   0.903    0.924  0.990   0.902  48.693"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BI-ENCODER % 10 >> CROSS-ENCODER\n",
    "cross_pipe = bi_enc % 10 >> cross_enc \n",
    "\n",
    "res = pt.Experiment(\n",
    "    [bm25, bi_enc, cross_pipe],\n",
    "    val_queries,\n",
    "    val_qrels,\n",
    "    names=[\"BM25\", \"Bi-Encoder\", \"Cross-Encoder\"],\n",
    "    eval_metrics=metrics,\n",
    ")\n",
    "display(res.round(3))\n",
    "\n",
    "del cross_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross encoder model should be trained.\n",
    "\n",
    "---\n",
    "\n",
    "# Causal LM\n",
    "\n",
    "```python\n",
    "input_str = f\"\"\"\n",
    "    Answer: {correct_answer}\n",
    "    Question: {question}\n",
    "    Is the Answer relevant to the Question? (Yes/No)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "We take as score the output of the model for the token associated to \"_Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b287bda8d14dce8fa20bb3cdd6fe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# QUANTO CONFIG\n",
    "quantization_config = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "# MODEL\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,  # Efficient data type for GPU\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_safetensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k_next_tokens(input_string, tokenizer=tokenizer, model=model, k=10):\n",
    "\n",
    "    # Tokenize to ids and move to the same device as the model\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Call model() to get logits\n",
    "    logits = model(input_ids)['logits']\n",
    "    print(\"shape of logits:\", logits.shape, \"\\n\")\n",
    "\n",
    "    # Only care about the last projection in the last batch\n",
    "    logits = logits[-1, -1]\n",
    "\n",
    "    # softmax() to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Keep only the top k\n",
    "    probs, ids = torch.topk(probs, k)\n",
    "\n",
    "    # Convert ids to tokens\n",
    "    texts = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    # Print\n",
    "    for prob, text in zip(probs, texts):\n",
    "        print(f\"{prob:.4f}: \\\"{text}\\\"\")\n",
    "\n",
    "    del input_ids, logits, probs, ids, texts\n",
    "    torch.cuda.empty_cache()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of logits: torch.Size([1, 11, 32064]) \n",
      "\n",
      "0.7862: \"▁night\"\n",
      "0.0335: \"night\"\n",
      "0.0197: \"▁day\"\n",
      "0.0158: \"▁rest\"\n",
      "0.0144: \"▁one\"\n",
      "0.0064: \"▁har\"\n",
      "0.0062: \"▁health\"\n",
      "0.0058: \"▁evening\"\n",
      "0.0045: \"▁hol\"\n",
      "0.0034: \"▁hunting\"\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Merry Christmas to all, and to all a good\"\n",
    "print_top_k_next_tokens(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for 'Yes': 8241\n",
      "Token ID for '▁Yes': 3869\n"
     ]
    }
   ],
   "source": [
    "true_token = \"Yes\"\n",
    "true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
    "print(f\"Token ID for '{true_token}': {true_id}\")\n",
    "\n",
    "token = tokenizer.tokenize(true_token)\n",
    "specific_token_id = tokenizer.encode(true_token, add_special_tokens=False)[0]\n",
    "print(f\"Token ID for '{token[0]}': {specific_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we do an example for our task.\n",
    "\n",
    "Give the Question and the Anwer, we ask if the Answer is relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of logits: torch.Size([1, 866, 32064]) \n",
      "\n",
      "0.3724: \"▁No\"\n",
      "0.1505: \"1\"\n",
      "0.1136: \"▁Yes\"\n",
      "0.0554: \"▁-\"\n",
      "0.0405: \"<0x0A>\"\n",
      "0.0325: \"2\"\n",
      "0.0224: \"▁[\"\n",
      "0.0197: \"▁no\"\n",
      "0.0185: \"<|end|>\"\n",
      "0.0144: \"▁Answer\"\n"
     ]
    }
   ],
   "source": [
    "sample = train_queries.head(1)\n",
    "sample_qrel = train_qrels[train_qrels['qid'] == sample['qid'].values[0]]\n",
    "correct_doc = corpus_df[corpus_df['docno'].isin(sample_qrel['docno'])]\n",
    "incorrect_docs = corpus_df[~corpus_df['docno'].isin(sample_qrel['docno'])].iloc[:5]\n",
    "\n",
    "question = sample['query_unprocessed'].iloc[0]\n",
    "correct_answer = correct_doc['text'].iloc[0]\n",
    "incorrect_answer = incorrect_docs['text'].iloc[0]\n",
    "\n",
    "input_string = f\"\"\"\n",
    "        Answer: {correct_answer}\n",
    "        Question: {question}\n",
    "        Is the Answer relevant to the Question? (Yes/No)\n",
    "    \"\"\"\n",
    "\n",
    "print_top_k_next_tokens(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_prob(question, answer, tokenizer=tokenizer, model=model):\n",
    "    # '_Yes' id is 3869\n",
    "    yes_id = 3869\n",
    "    input_string = f\"\"\"\n",
    "        Answer: {answer}\n",
    "        Question: {question}\n",
    "        Is the Answer relevant to the Question? (Yes/No)\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(model.device)\n",
    "    logits = model(input_ids)['logits']\n",
    "    logits = logits[-1, -1]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the 'Yes' probability\n",
    "    yes_prob = probs[yes_id].item()\n",
    "\n",
    "    del input_ids, logits, probs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return yes_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes probability for the correct answer: 0.1136\n",
      "Yes probability for the incorrect answer: 0.0775\n"
     ]
    }
   ],
   "source": [
    "correct_prob = get_yes_prob(question, correct_answer)\n",
    "incorrect_prob = get_yes_prob(question, incorrect_answer)\n",
    "\n",
    "print(f\"Yes probability for the correct answer: {correct_prob:.4f}\")\n",
    "print(f\"Yes probability for the incorrect answer: {incorrect_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_df is indexed by 'docno'\n",
    "corpus_df_docno = corpus_df.set_index('docno')\n",
    "\n",
    "def _get_YesLLM_scores_single(df, tokenizer, model, yes_id=3869, query_field='query_unprocessed', corpus_df=corpus_df_docno, max_doc_length=512):\n",
    "    \"\"\"\n",
    "    Get probability for '_Yes' token for a (question, answer) pairs.\n",
    "\n",
    "    This will be used as an argument for pyterrier.apply.doc_score().\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the question and answer texts\n",
    "    query_text = df[query_field]\n",
    "    docno = df['docno']\n",
    "    doc_text = corpus_df.loc[docno]['text']\n",
    "    doc_text = doc_text.split()\n",
    "    doc_text = \" \".join(doc_text[:max_doc_length])\n",
    "\n",
    "    # Get the probability for the 'Yes' token\n",
    "    yes_prob = get_yes_prob(query_text, doc_text, tokenizer=tokenizer, model=model)\n",
    "\n",
    "    return yes_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BM25\n",
    "path = \"./index_sepqa/index_bm25/data.properties\"\n",
    "bm25_index = pt.IndexFactory.of(path)\n",
    "\n",
    "bm25 = pt.terrier.Retriever(\n",
    "    bm25_index, \n",
    "    wmodel=\"BM25\", \n",
    "    controls={'c': 1.0, 'bm25.k_1': 2.5},\n",
    "    properties={\"termpipelines\": \"\"},\n",
    ")\n",
    "\n",
    "### BI-ENCODER\n",
    "index_path = \"./index_sepqa/MiniLM_faiss_IndexFlatIP.index\"\n",
    "faiss_index = faiss.read_index(index_path)\n",
    "biencoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "transform_biencoder = partial(_transform_biencoder, FAISS_INDEX=faiss_index, biencoder_model=biencoder_model, k=100, index_docno_dict=index_docno_dict)\n",
    "bi_enc = pt.apply.generic(transform_biencoder)\n",
    "\n",
    "### LLM SCORE\n",
    "# llm_score = partial(_get_YesLLM_scores, model=model, tokenizer=tokenizer)\n",
    "# llm_score_transformer = pt.apply.doc_score(llm_score, batch_size=16)\n",
    "llm_score = partial(_get_YesLLM_scores_single, model=model, tokenizer=tokenizer)\n",
    "llm_score_transformer = pt.apply.doc_score(llm_score)\n",
    "pipeline = bi_enc % 10 >> llm_score_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>AP@100</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.815</td>\n",
       "      <td>17.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-Encoder</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.941</td>\n",
       "      <td>12.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM rerank</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.259</td>\n",
       "      <td>2828.864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    P@1    P@3  nDCG@3  nDCG@10  R@100  AP@100       mrt\n",
       "0        BM25  0.755  0.286   0.814    0.841  0.969   0.815    17.491\n",
       "1  Bi-Encoder  0.918  0.316   0.938    0.952  1.000   0.941    12.386\n",
       "2  LLM rerank  0.071  0.085   0.175    0.425  0.990   0.259  2828.864"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [P@1, P@3, nDCG@3, nDCG@10, R@100, MAP@100, 'mrt']\n",
    "\n",
    "res = pt.Experiment(\n",
    "    [bm25, bi_enc, pipeline],\n",
    "    val_queries,\n",
    "    val_qrels,\n",
    "    names=[\"BM25\", \"Bi-Encoder\", \"LLM rerank\"],\n",
    "    eval_metrics=metrics,\n",
    ")\n",
    "\n",
    "display(res.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea is fun, but in this raw way is not useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
